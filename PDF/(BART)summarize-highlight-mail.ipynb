{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02fe7fab-f8f5-4623-807f-4b9ce7b03f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset \n",
    "from transformers import BertTokenizer, BartForConditionalGeneration, BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80dd0791-a11a-43e0-a82f-0e54336006de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0+cu124'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2579c5fd-cc09-46af-94ee-b1867b3977d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    # \"mps\"  if torch.backends.mps.is_available() and torch.backends.mps.is_built()else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e76ddebb-a639-4fc9-b3d4-1110369346f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4090'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb43ff3d-204e-4f96-bc6e-f182e8f0f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_with_spacy(text, max_tokens=512):\n",
    "    \"\"\"\n",
    "    這是利用 spacy 的方式將句子切割放入 chunk，讓每一個 chunk 的總 token 數 <= max_tokens。\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        sentence = sent.text\n",
    "        if len(tokenizer(current_chunk + sentence)['input_ids']) > max_tokens:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "        else:\n",
    "            current_chunk += \" \" + sentence\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "badd8b78-3aae-4393-bdcf-43268321737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_summary_punctuation(summary):\n",
    "    \"\"\"\n",
    "    這是用來處理，如果結尾沒有標點符號，則加入標點符號。\n",
    "    \"\"\"\n",
    "    if summary and summary[-1] not in \".!?\":\n",
    "        return summary + \".\"\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296227cb-76c5-4884-b74a-f6dea22b54c7",
   "metadata": {},
   "source": [
    "### **這裡是用來載入模型的**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aa4326d-a272-4c26-8add-3caecf615d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb0cb651-33f2-42c3-a714-cac131f7925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(article, min_summary_length=130, max_summary_length=512):\n",
    "    \"\"\"\n",
    "    這是寫的，這裡一開始encode的max_length用1024，因為這是 bart 最大輸入token 數限制。\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(article, max_length=1024, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    # inputs = tokenizer(chunk, return_tensors=\"pt\", max_length=512, truncation=True).to(device)['input_ids']\n",
    "    summary_ids = model.generate(input_ids,\n",
    "                                 length_penalty=2.0, num_beams=4, early_stopping=True,\n",
    "                                 min_length=min_summary_length,\n",
    "                                 max_length=max_summary_length)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def count_token(text):\n",
    "    return len(tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=True)[0]) # goal_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f95da24-3553-487c-b7b2-8b33af110b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_summaries(summaries, max_tokens=512):\n",
    "    merged_chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for summary in summaries:\n",
    "        if count_token(current_chunk + \" \" + summary) > max_tokens:\n",
    "            merged_chunks.append(current_chunk.strip())\n",
    "            current_chunk = summary\n",
    "        else:\n",
    "            current_chunk += \" \" + summary\n",
    "    if current_chunk:\n",
    "        merged_chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return merged_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a441342-b3c5-4ead-912b-46fee42524e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bart_summarize_article(\n",
    "        article,\n",
    "        max_bart_input_length=512,\n",
    "        min_summary_length=128,\n",
    "        max_summary_length=512,\n",
    "        final_min_summary_length=128,\n",
    "        final_max_summary_length=256,\n",
    "        log=False\n",
    "    ):\n",
    "    \"\"\"使用 BART 模型對給定的長文本文章進行摘要。\n",
    "\n",
    "    這個函數將文章分割成多個部分，並對每個部分生成摘要，然後合併這些摘要，直到只剩下一個摘要為止。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    article (str)-------------------- 要摘要的文章內容。\n",
    "    max_bart_input_length (int)------ BART 模型輸入的最大長度（以 token 計算），默認為 512。\n",
    "    min_summary_length (int)--------- 生成摘要的最小長度（以 token 計算），默認為 128。\n",
    "    max_summary_length (int)--------- 生成摘要的最大長度（以 token 計算），默認為 512。\n",
    "    final_min_summary_length (int)--- 最終摘要的最小長度（以 token 計算），默認為 128。\n",
    "    final_max_summary_length (int)--- 最終摘要的最大長度（以 token 計算），默認為 256。\n",
    "    log (bool)----------------------- 是否打印每一層的摘要過程和 token 數的日誌，默認為 False。\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    summarized_article (str): 生成的最終摘要內容。\n",
    "    \"\"\"\n",
    "    # --- 將原始 article 按照 max_tokens 分割成好幾個 parts。（原因：因為 bart 輸入的 token 數只能是 1024。）\n",
    "    article_parts = split_text_with_spacy(article, max_tokens=max_bart_input_length)\n",
    "    \n",
    "    # --- 開始使用 while 迴圈，一層一層不斷切割，並摘要文章。\n",
    "    round = 1 # 用於記錄 層數/圈數，只會在 log 裡面出現，如果要保守點預防無限迴圈，應該要把他加入到 while 判斷，如果 round > 某個數，就 break。\n",
    "    while len(article_parts) > 1: # \n",
    "        # --- 這裡是用來 print 出 token 數做紀錄的\n",
    "        if log:\n",
    "            print(f\"\\noriginal layer-{round:<2}({len(article_parts):<2} parts): {' '*round*3}|\", end='')\n",
    "            for idx, article_part in enumerate(article_parts): print(f\" {count_token(article_part):<4}|\", end='')\n",
    "            print(f\"\\n summary layer-{round:<2}({len(article_parts):<2} parts): {' '*round*3}|\", end='')\n",
    "        # --- 這裡開始用 for 迴圈跑，對於每一個文章，我都要進行一次摘要。\n",
    "        summaries = [] # 用於記錄所有的摘要結果\n",
    "        for idx, article_part in enumerate(article_parts):\n",
    "            summary = generate_summary(article_part, # 對於每一個文章根據限制數量進行摘要\n",
    "                                       min_summary_length=min_summary_length, # 限制摘要常最短為 min_summary_length\n",
    "                                       max_summary_length=max_summary_length  # 限制摘要常最長為 max_summary_length\n",
    "                                      )\n",
    "            summary = fix_summary_punctuation(summary) # 由於有可能生成的結果沒有結尾符號，所以這裡加上去。\n",
    "            summaries.append(summary) # 將輸出的摘要記錄。\n",
    "            # --- 這裡是用來 print 出紀錄的\n",
    "            if log: print(f\" {count_token(summary):<4}|\", end='')\n",
    "            # ---\n",
    "\n",
    "        # --- 將 article_parts 按照 max_tokens 組合成好幾個 parts。（原因：因為 bart 輸入的 token 數只能是 1024。）\n",
    "        article_parts = merge_summaries(summaries, max_tokens=max_bart_input_length)\n",
    "        round += 1\n",
    "    # --- 生成對後一次的摘要\n",
    "    final_summary = generate_summary(article_parts[0], # 因為前面 while 迴圈的關係，這裡的 article_parts 長度一定會是 1，所以只取第一個當作輸入。\n",
    "                                     min_summary_length=final_min_summary_length, # 限制摘要常最短為 final_min_summary_length\n",
    "                                     max_summary_length=final_max_summary_length  # 限制摘要常最長為 final_max_summary_length\n",
    "                                    )\n",
    "    # --- 這裡是用來 print 出 token 數做紀錄的\n",
    "    if log:\n",
    "        print(f\"\\noriginal layer-{round:<3}({1:<2} parts): {' '*round*3}| {count_token(article_parts[0]):<4}|\", end='')\n",
    "        print(f\"\\nsummary  layer-{round:<3}({1:<2} parts): {' '*round*3}| {count_token(final_summary):<4}|\", end='')\n",
    "    # ---\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb68c3df-b7da-434a-ad60-eca38d32fb36",
   "metadata": {},
   "source": [
    "## 這裡用 arXiv 抓下來的文章做測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aad05f4-ff92-46d8-a9ad-81d9adcbc268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original layer-1 (13 parts):    | 500 | 473 | 489 | 500 | 505 | 508 | 497 | 505 | 459 | 507 | 461 | 411 | 478 |\n",
      " summary layer-1 (13 parts):    | 245 | 255 | 245 | 246 | 238 | 241 | 242 | 256 | 237 | 240 | 231 | 239 | 256 |\n",
      "original layer-2 (7  parts):       | 498 | 489 | 477 | 495 | 475 | 468 | 256 |\n",
      " summary layer-2 (7  parts):       | 239 | 245 | 240 | 241 | 234 | 253 | 256 |\n",
      "original layer-3 (4  parts):          | 482 | 479 | 485 | 256 |\n",
      " summary layer-3 (4  parts):          | 249 | 254 | 238 | 256 |\n",
      "original layer-4 (2  parts):             | 501 | 492 |\n",
      " summary layer-4 (2  parts):             | 248 | 248 |\n",
      "original layer-5  (1  parts):                | 494 |\n",
      "summary  layer-5  (1  parts):                | 286 |"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Coarse maps of the distribution of galaxies can be constructed by measuring redshifts. Results of the study will be published in the forthcoming issue of The Astrophysics of Light and Space, published by The Astronomical Society of America. The entire Universe can be considered a patchwork of abutting BoAs, just as the terrestrial landscape is separated into watersheds. A BoA is generally not gravitationally bound, as the relative motion of distant points within it are usually dominated by the cosmic expansion. Any source position in a BoA leads via a streamline to a \"sink\" near the potential minimum within the BoA. Streamlines diverge out of the local maxima of the velocity potential and converge onto its local minima - namely they \\'stream\\' away from the underdense to the dense regions of the Universe. Overall, there is reasonable coverage outside the zone of Milky Way obscuration across the sky within $z=0.05$ with a slight deficiency south of the Milky Way in the celestial north. Scatter in distance and velocity measurements can be averaged over members of a galaxy group or cluster reducing errors. We will continuously update the calculator for better visualizations and to incorporate the latest improvements. The estimated density and velocity fields are available upon reasonable request to the authors. We also offer an online tool that correlates distance and velocities using the constructed velocity field within this program.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_article = open(\"./test/article.md\").read()\n",
    "test_abstract = open(\"./test/abstract.md\").read()\n",
    "goal_token_count = count_token(test_abstract)\n",
    "bart_summarize_article(test_article,\n",
    "                  max_bart_input_length    = 512,\n",
    "                  min_summary_length       = 230, # ~ (512/2) * 0.9\n",
    "                  max_summary_length       = 256, # ~ (512/2) \n",
    "                  final_min_summary_length = int(goal_token_count*0.9),\n",
    "                  final_max_summary_length = int(goal_token_count*1.2),\n",
    "                  log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f38688d2-732d-4968-9316-e5826e1a28a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bart_summarize_article(\n",
    "        article,\n",
    "        max_bart_input_length=512,\n",
    "        min_summary_length=128,\n",
    "        max_summary_length=512,\n",
    "        final_min_summary_length=128,\n",
    "        final_max_summary_length=256\n",
    "    ):\n",
    "    \n",
    "    article_parts = split_text_with_spacy(article, max_tokens=max_bart_input_length)\n",
    "    \n",
    "    while len(article_parts) > 1: \n",
    "        summaries = [] \n",
    "        for idx, article_part in enumerate(article_parts):\n",
    "            summary = generate_summary(article_part, \n",
    "                                       min_summary_length=min_summary_length, \n",
    "                                       max_summary_length=max_summary_length  \n",
    "                                      )\n",
    "            summary = fix_summary_punctuation(summary) \n",
    "            summaries.append(summary) \n",
    "        article_parts = merge_summaries(summaries, max_tokens=max_bart_input_length)\n",
    "    \n",
    "    final_summary = generate_summary(article_parts[0], \n",
    "                                     min_summary_length=final_min_summary_length, \n",
    "                                     max_summary_length=final_max_summary_length  \n",
    "                                    )\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff4023-9559-46d0-8790-49b93799895e",
   "metadata": {},
   "source": [
    "## 這裡使用 ollama 生成的文章做 bart 摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ce02718-2359-47ac-a28c-fb893a585df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11490, 11490)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# task_name = 'original'\n",
    "# task_name = 'highlighted'\n",
    "# task_name = 'summarized'\n",
    "# task_name = 'compressed'\n",
    "# task_name = 'abstracted'\n",
    "# task_name = 'ollama_highlight_token_ratio'\n",
    "# task_name = 'ollama_highlight_words'\n",
    "task_name = 'ollama_highlight_words_ratio'\n",
    "# ---\n",
    "if task_name=='original':\n",
    "    df_by_llama = pd.read_csv(\"./dat/cnn_dailymail_test.csv\")\n",
    "else:\n",
    "    df_by_llama = pd.read_csv(f\"./dat/cnn_dailymail_test-{task_name}.csv\")\n",
    "# ---\n",
    "df_token_count = pd.read_csv(f\"./dat/cnn_dailymail_test-token_count.csv\")\n",
    "df_by_llama.shape[0], df_token_count.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bb14991-4d8f-45d9-96b8-ae4b8fc824be",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_ids = df_by_llama['id']\n",
    "article_need_summarize = df_by_llama['article']\n",
    "highlight_token_counts = df_token_count['highlights']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36953dc6-bb6c-4d52-ba17-17d9e0a0d1c7",
   "metadata": {},
   "source": [
    "## **這裡使用的 min length 和 max length 是 CNN daily mail 中 highlights 的 token 數量 $\\pm 30\\%$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d17109d3-86cf-48de-82b1-8a16c26f040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# time.sleep(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7d6e052-db28-4650-83b3-5a7a6ffa2c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BART summarize: 100%|██████████████████████████████████████████████████████████| 11490/11490 [4:35:08<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarize complete => save to csv complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SAVE_RESULT_PATH = f'./dat/cnn_dailymail_test-{task_name}-bart.dat'\n",
    "SAVE_EVERRY_NUM = 5 # 每跑完幾筆資料就儲存\n",
    "TEMPORARY_SAVE_STRING = \"\" # 用於儲存要放入文件的字串\n",
    "\n",
    "processed_index = 0\n",
    "summarized_articles = []\n",
    "for article_id, goal_token_count, article in tqdm(zip(article_ids, highlight_token_counts, article_need_summarize), total=len(article_ids), desc='BART summarize'):\n",
    "    try:\n",
    "        summarized_article = bart_summarize_article(article,\n",
    "                  max_bart_input_length    = 512,\n",
    "                  min_summary_length       = 230, # ~ (512/2) * 0.9\n",
    "                  max_summary_length       = 256, # ~ (512/2) \n",
    "                  final_min_summary_length = goal_token_count,\n",
    "                  final_max_summary_length = int(goal_token_count*1.2))\n",
    "        TEMPORARY_SAVE_STRING += f\"{article_id},{json.dumps(summarized_article)}\\n\"\n",
    "        if not processed_index%SAVE_EVERRY_NUM :\n",
    "            with open(SAVE_RESULT_PATH, \"a\") as file:\n",
    "                file.write(TEMPORARY_SAVE_STRING)\n",
    "            TEMPORARY_SAVE_STRING = \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {processed_index}: {e}\")\n",
    "    processed_index += 1\n",
    "if TEMPORARY_SAVE_STRING:\n",
    "    with open(SAVE_RESULT_PATH, \"a\") as file:\n",
    "        file.write(TEMPORARY_SAVE_STRING)\n",
    "print(\"summarize complete => save to csv complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce92c63e-dfd9-4a5c-bf46-6a79f67d7319",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEMPORARY_SAVE_STRING:\n",
    "    with open(SAVE_RESULT_PATH, \"a\") as file:\n",
    "        file.write(TEMPORARY_SAVE_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730cb9b-7470-4794-9561-6d56f8f071a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
