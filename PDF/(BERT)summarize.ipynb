{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "291669e0-7552-4d86-855d-6ff84eecf635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from summarizer import Summarizer\n",
    "from rouge_score import rouge_scorer\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "fcdcdf60-91a1-4dbf-b3af-085a07f267a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 4])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array([1,2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430a7618-28f7-49e9-9f90-54cf8aa4b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a5e5f69-4ccd-4188-923c-17f400ce2526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yangchangmao/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "BERT_summarize = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf456de-9ec1-478c-a438-3bf731bce75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dat/cnn_dailymail_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cbf567cd-d9b7-4bd2-bc22-b68fb7432ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_scores(reference_text, generated_text):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_text, generated_text)\n",
    "    rouge_1 = scores['rouge1'].fmeasure\n",
    "    rouge_2 = scores['rouge2'].fmeasure\n",
    "    rouge_L = scores['rougeL'].fmeasure\n",
    "    return rouge_1, rouge_2, rouge_L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de566405-2c38-4613-b2ba-488575963b8f",
   "metadata": {},
   "source": [
    "# BERT summarize testing\n",
    "\n",
    "**Following is testing for article summairze by BERT and do ROUGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cff78d9e-dc05-4971-8093-7fb83e12570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import random\n",
    "from numpy import exp,array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7540d32b-7666-4d18-aac4-ce4791d7c212",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[143], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m _rouge \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num_sent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(goal_num_sents \u001b[38;5;241m-\u001b[39m width_num_sents, \u001b[38;5;28mint\u001b[39m(goal_num_sents \u001b[38;5;241m+\u001b[39m width_num_sents\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# ---\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     bert_summarized_article \u001b[38;5;241m=\u001b[39m \u001b[43mBERT_summarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_article\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_sentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_sent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     bert_summarized_article_dict[num_sent] \u001b[38;5;241m=\u001b[39m bert_summarized_article\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# ---\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/summarizer/summary_processor.py:234\u001b[0m, in \u001b[0;36mSummaryProcessor.__call__\u001b[0;34m(self, body, ratio, min_length, max_length, use_first, algorithm, num_sentences, return_as_list)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m     body: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     return_as_list: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    220\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    (utility that wraps around the run function)\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m    Preprocesses the sentences, runs the clusters to find the centroids, then combines the sentences.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m    :return: A summary sentence.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m                    \u001b[49m\u001b[43muse_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_as_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/summarizer/summary_processor.py:202\u001b[0m, in \u001b[0;36mSummaryProcessor.run\u001b[0;34m(self, body, ratio, min_length, max_length, use_first, algorithm, num_sentences, return_as_list)\u001b[0m\n\u001b[1;32m    199\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentence_handler(body, min_length, max_length)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sentences:\n\u001b[0;32m--> 202\u001b[0m     sentences, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_sentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_as_list:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentences\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/summarizer/summary_processor.py:108\u001b[0m, in \u001b[0;36mSummaryProcessor.cluster_runner\u001b[0;34m(self, sentences, ratio, algorithm, use_first, num_sentences)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03mRuns the cluster algorithm based on the hidden state. Returns both the embeddings and sentences.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m:return: A tuple of summarized sentences and embeddings\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m first_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_first:\n\u001b[1;32m    111\u001b[0m     num_sentences \u001b[38;5;241m=\u001b[39m num_sentences \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_sentences \u001b[38;5;28;01melse\u001b[39;00m num_sentences\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/summarizer/transformer_embeddings/bert_embedding.py:173\u001b[0m, in \u001b[0;36mBertEmbedding.__call__\u001b[0;34m(self, content, hidden, reduce_option, hidden_concat)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    159\u001b[0m     content: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m     hidden_concat: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    163\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ndarray:\n\u001b[1;32m    164\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    Create matrix from the embeddings.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    :return: A numpy array matrix of the given content.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_option\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_concat\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/summarizer/transformer_embeddings/bert_embedding.py:151\u001b[0m, in \u001b[0;36mBertEmbedding.create_matrix\u001b[0;34m(self, content, hidden, reduce_option, hidden_concat)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_matrix\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     content: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     hidden_concat: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    140\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ndarray:\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    Create matrix from the embeddings.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    :return: A numpy array matrix of the given content.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([\n\u001b[1;32m    152\u001b[0m         np\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_embeddings(\n\u001b[1;32m    153\u001b[0m             t, hidden\u001b[38;5;241m=\u001b[39mhidden, reduce_option\u001b[38;5;241m=\u001b[39mreduce_option, hidden_concat\u001b[38;5;241m=\u001b[39mhidden_concat\n\u001b[1;32m    154\u001b[0m         )\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m content\n\u001b[1;32m    155\u001b[0m     ])\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/summarizer/transformer_embeddings/bert_embedding.py:152\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_matrix\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     content: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     hidden_concat: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    140\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ndarray:\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    Create matrix from the embeddings.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    :return: A numpy array matrix of the given content.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([\n\u001b[0;32m--> 152\u001b[0m         np\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m            \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_option\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_option\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_concat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_concat\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m content\n\u001b[1;32m    155\u001b[0m     ])\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/summarizer/transformer_embeddings/bert_embedding.py:108\u001b[0m, in \u001b[0;36mBertEmbedding.extract_embeddings\u001b[0;34m(self, text, hidden, reduce_option, hidden_concat)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03mExtracts the embeddings for the given text.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m:return: A torch vector.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m tokens_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_input(text)\n\u001b[0;32m--> 108\u001b[0m pooled, hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens_tensor\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# deprecated temporary keyword functions.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce_option \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconcat_last_4\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:286\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    278\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    285\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 286\u001b[0m     mixed_query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m# If this is instantiated as a cross-attention module, the keys\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# and values come from an encoder; the attention mask needs to be\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;66;03m# such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     is_cross_attention \u001b[38;5;241m=\u001b[39m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_ids = df['id']\n",
    "cnn_articles = df['article']\n",
    "cnn_higlights = df['highlights']\n",
    "\n",
    "ERROR_index = []\n",
    "\n",
    "bert_summarized_articles = {}\n",
    "rouge_bert_summarized_articles = {}\n",
    "\n",
    "fig = plt.figure(figsize = (7,3))\n",
    "ax = fig.subplots()\n",
    "max_num = 100\n",
    "# ---\n",
    "max_width_num_sents = 3\n",
    "_rouges = []\n",
    "for i in range(max_num):\n",
    "    try:\n",
    "        cnn_id = cnn_ids[i]\n",
    "        cnn_article = cnn_articles[i]\n",
    "        cnn_higlight = cnn_higlights[i]\n",
    "        goal_num_sents = len(list(nlp(cnn_higlight).sents))\n",
    "        width_num_sents = min(goal_num_sents - 1, max_width_num_sents)\n",
    "        # ---\n",
    "        bert_summarized_article_dict = {}\n",
    "        rouge_bert_summarized_article_dict = {}\n",
    "        \n",
    "        _rouge = []\n",
    "        for num_sent in range(goal_num_sents - width_num_sents, int(goal_num_sents + width_num_sents+1)):\n",
    "            # ---\n",
    "            bert_summarized_article = BERT_summarize(cnn_article, num_sentences=num_sent)\n",
    "            bert_summarized_article_dict[num_sent] = bert_summarized_article\n",
    "            # ---\n",
    "            rouge_bert_summarized_article = calculate_rouge_scores(cnn_higlight, bert_summarized_article) \n",
    "            rouge_bert_summarized_article_dict[num_sent] = rouge_bert_summarized_article\n",
    "            # rouge_bert_summarized_article = [exp(-0.2*(num_sent - width_num_sents-1)**2) + 0.2*random()]\n",
    "            # time.sleep(0.1)\n",
    "            # ---\n",
    "            for all_rouge in _rouges:\n",
    "                len_rouge = len(all_rouge)\n",
    "                ax.plot(range(-int(len_rouge/2),-int(len_rouge/2)+len_rouge), all_rouge,\n",
    "                        '-o', markersize=1, linewidth=0.8, color= 'red', alpha=min(1.0, 10/i))\n",
    "            # ---\n",
    "            _rouge.append(rouge_bert_summarized_article[0])\n",
    "            ax.plot(range(-width_num_sents,-width_num_sents+num_sent), _rouge, '-o', linewidth=1, color= 'blue', alpha=1)\n",
    "            ax.axvline(x=0, color='gray')\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xticks([])\n",
    "            display(plt.gcf())\n",
    "            plt.cla()\n",
    "            print(f'Process num: {i+1}/{max_num}')\n",
    "            clear_output(wait=True)\n",
    "            # ---\n",
    "        _rouges.append(_rouge)\n",
    "        # ---\n",
    "        bert_summarized_articles[cnn_id] = (bert_summarized_article_dict)\n",
    "        rouge_bert_summarized_articles[cnn_id] = (rouge_bert_summarized_article_dict)\n",
    "    except Exception as e:\n",
    "        ERROR_index.append(i)\n",
    "        print(f\"Error in {i}: {e}\")\n",
    "\n",
    "print('Calculate complete!')\n",
    "\n",
    "for _rouge in _rouges:\n",
    "    len_rouge = len(_rouge)\n",
    "    ax.plot(range(-int(len_rouge/2),-int(len_rouge/2)+len_rouge), _rouge,\n",
    "            '-o', markersize=1, linewidth=0.8, color= 'red', alpha=min(1.0, 10/(max_num-1)))\n",
    "\n",
    "with open('./dat/cnn_dailymail_test-BERT.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(bert_summarized_articles, f, ensure_ascii=False, indent=1)\n",
    "\n",
    "with open('./dat/cnn_dailymail_test-BERT-rouge_scores.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(rouge_bert_summarized_articles, f, ensure_ascii=False, indent=1)\n",
    "\n",
    "print(\"Write to file complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f1e6a988-bdbb-4fbe-add7-d755039cc2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./dat/cnn_dailymail_test-BERT.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(bert_summarized_articles, f, ensure_ascii=False, indent=1)\n",
    "\n",
    "# with open('./dat/cnn_dailymail_test-BERT-rouge_scores.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(rouge_bert_summarized_articles, f, ensure_ascii=False, indent=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
