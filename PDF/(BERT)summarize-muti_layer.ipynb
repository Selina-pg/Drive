{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "291669e0-7552-4d86-855d-6ff84eecf635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from summarizer import Summarizer\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "430a7618-28f7-49e9-9f90-54cf8aa4b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "BERT_summarize = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "caf456de-9ec1-478c-a438-3bf731bce75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dat/cnn_dailymail_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0a5e5f69-4ccd-4188-923c-17f400ce2526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_token(text):\n",
    "    return len(tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=True)[0])\n",
    "def count_sents(text):\n",
    "    return len(list(nlp(text).sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "55bee7bf-8911-4ee8-bdc8-fb4279429aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_with_spacy(text, max_num_sents=4):\n",
    "    sents = list(nlp(text).sents)\n",
    "    chunks = []\n",
    "    for i in range(0, len(sents), max_num_sents):\n",
    "        chunks.append(' '.join([str(sent) for sent in sents[i:i+max_num_sents]]))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c0d1d60d-610e-4ead-b437-ea6dd816b474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_summaries(summaries, max_num_sents=2):\n",
    "    merged_chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for summary in summaries:\n",
    "        if count_sents(current_chunk+' '+summary) > max_num_sents:\n",
    "            merged_chunks.append(current_chunk.strip())\n",
    "            current_chunk = summary\n",
    "        else:\n",
    "            current_chunk += \" \" + summary\n",
    "    if current_chunk:\n",
    "        merged_chunks.append(current_chunk.strip())\n",
    "    return merged_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b4786c39-f338-4e54-9c58-7a0242df8abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_summarize_article_with_log(\n",
    "        article,\n",
    "        max_bert_input_length=4, # sentences\n",
    "        step_summary_length=2, # sentences\n",
    "        final_summary_length=2, # sentences\n",
    "        log=False\n",
    "    ):\n",
    "    article_parts = split_text_with_spacy(article, max_num_sents=max_bert_input_length)\n",
    "    num_previous_part = len(article_parts)\n",
    "    round=1\n",
    "    while len(article_parts) > 1: \n",
    "        if log:\n",
    "            print(f\"\\noriginal layer-{round:<2}({len(article_parts):<2} parts): {' '*round*3}|\", end='')\n",
    "            for idx, article_part in enumerate(article_parts): print(f\"{count_sents(article_part):>2} |\", end='')\n",
    "            print(f\"\\n summary layer-{round:<2}({len(article_parts):<2} parts): {' '*round*3}|\", end='')\n",
    "        # ---\n",
    "        summaries = [] \n",
    "        for idx, article_part in enumerate(article_parts):\n",
    "            summary = BERT_summarize(article_part, num_sentences=step_summary_length)\n",
    "            summaries.append(summary)\n",
    "            # ---\n",
    "            if log: print(f\"{count_sents(summary):>2} |\", end='')\n",
    "        article_parts = merge_summaries(summaries, max_num_sents=max_bert_input_length)\n",
    "        num_current_part = len(article_parts)\n",
    "        if(num_previous_part==num_current_part):\n",
    "            break\n",
    "        num_previous_part = num_current_part\n",
    "        round+=1\n",
    "    # ---\n",
    "    if log: print(f\"\\noriginal layer-{round:<3}({1:<2} parts): {' '*round*3}|{count_sents(article_parts[0]):>2} |\", end='')\n",
    "    # ---\n",
    "    final_summary = BERT_summarize(article_parts[0], num_sentences=final_summary_length)\n",
    "    # ---\n",
    "    if log: print(f\"\\nsummary  layer-{round:<3}({1:<2} parts): {' '*round*3}|{count_sents(final_summary):>2} |\", end='')\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b7e6420a-c370-4c58-86b8-8e1b175c0876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original layer-1 (21 parts):    | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 |\n",
      " summary layer-1 (21 parts):    | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 3 | 2 | 2 |\n",
      "original layer-2 (11 parts):       | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 3 | 4 |\n",
      " summary layer-2 (11 parts):       | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 3 | 3 |\n",
      "original layer-3 (7  parts):          | 4 | 4 | 4 | 4 | 2 | 3 | 3 |\n",
      " summary layer-3 (7  parts):          | 2 | 2 | 2 | 2 | 2 | 3 | 3 |\n",
      "original layer-4 (5  parts):             | 4 | 4 | 2 | 3 | 3 |\n",
      " summary layer-4 (5  parts):             | 2 | 2 | 2 | 3 | 3 |\n",
      "original layer-5 (4  parts):                | 4 | 2 | 3 | 3 |\n",
      " summary layer-5 (4  parts):                | 2 | 2 | 3 | 3 |\n",
      "original layer-6 (3  parts):                   | 4 | 3 | 3 |\n",
      " summary layer-6 (3  parts):                   | 2 | 3 | 3 |\n",
      "original layer-6  (1  parts):                   | 2 |\n",
      "summary  layer-6  (1  parts):                   | 2 |"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'(CNN)James Holmes made his introduction to the world in a Colorado cinema filled with spectators watching a midnight showing of the new Batman movie, \"The Dark Knight Rises,\" in June 2012. According to The New York Times, Holmes sent a text message to a fellow graduate student, a woman, about two weeks before the shooting.'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_summarize_article_with_log(df['article'][719], log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "02c6b524-b784-4265-aa5d-c46246daf24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_summarize_article(\n",
    "        article,\n",
    "        max_bert_input_length=4, # sentences\n",
    "        step_summary_length=2, # sentences\n",
    "        final_summary_length=2, # sentences\n",
    "    ):\n",
    "    article_parts = split_text_with_spacy(article, max_num_sents=max_bert_input_length)\n",
    "    num_previous_part = len(article_parts)\n",
    "    while len(article_parts) > 1: \n",
    "        summaries = [] \n",
    "        for idx, article_part in enumerate(article_parts):\n",
    "            summary = BERT_summarize(article_part, num_sentences=step_summary_length)\n",
    "            summaries.append(summary)\n",
    "        article_parts = merge_summaries(summaries, max_num_sents=max_bert_input_length)\n",
    "        num_current_part = len(article_parts)\n",
    "        if(num_previous_part==num_current_part): break\n",
    "        num_previous_part = num_current_part\n",
    "    final_summary = BERT_summarize(article_parts[0], num_sentences=final_summary_length)\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd48f0-7bca-414d-81a5-c35c24e558b6",
   "metadata": {},
   "source": [
    "# Estimate the ratio between sentence and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ffa38fb9-2248-4f5d-873c-fd06d02a0dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "max_length = len(df['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c8728786-e2e7-4fb0-890d-2148eff0616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_token_num_a = []\n",
    "cnn_sents_num_a = []\n",
    "for article in tqdm(df['article'], bar_format='{desc:<5.5}{percentage:3.0f}%|{bar:50}{r_bar}'):\n",
    "    cnn_token_num_a.append(count_token(article))\n",
    "    cnn_sents_num_a.append(count_sents(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e2273-0571-4893-8752-58242d27ab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.means(cnn_token_num_a/cnn_sents_num_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "13cb6892-de45-48c0-8869-f0929b98c027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     100%|██████████████████████████████████████████████████| 11490/11490 [02:01<00:00, 94.45it/s]\n"
     ]
    }
   ],
   "source": [
    "cnn_token_num_h = []\n",
    "cnn_sents_num_h = []\n",
    "for article in tqdm(df['highlights'], bar_format='{desc:<5.5}{percentage:3.0f}%|{bar:50}{r_bar}'):\n",
    "    cnn_token_num_h.append(count_token(article))\n",
    "    cnn_sents_num_h.append(count_sents(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "36d8ff5d-fab7-4f36-801c-701499f47279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 13)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(512/19.27), int(256/19.27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d4aedb46-8fb3-4c95-93f3-8037ccb27673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.279391011018365\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratio = np.array(cnn_token_num_h)/np.array(cnn_sents_num_h)\n",
    "mean = np.mean(ratio)\n",
    "print(mean)\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.hist(ratio, bins=200)\n",
    "ax.axvline(x=mean, color='red')\n",
    "ax.set_xlim(0,2*mean)\n",
    "ax.set_xlabel('token / sentences')\n",
    "ax.set_ylabel('#')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473d7a5c-4d7d-4fcd-a108-278ce49fbe70",
   "metadata": {},
   "source": [
    "- mean of ${\\color{red}\\mathrm{Token}/\\mathrm{Sentence}=19.279391011018365{\\,\\mathrm{[token/sents.]}}}$ (in highlights)\n",
    "- $\\displaystyle \\mathrm{floor}\\left(\\frac{512 {\\rm token}}{19.2794\\,\\mathrm{[token/sents.]}}\\right) = 26$\n",
    "- $\\displaystyle \\mathrm{floor}\\left(\\frac{256 {\\rm token}}{19.2794\\,\\mathrm{[token/sents.]}}\\right) = 13$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "d8d24ad9-a2e7-4088-9b96-2cd730d28a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original layer-1 (4  parts):    |26 |26 |26 | 6 |\n",
      " summary layer-1 (4  parts):    |13 |13 |14 | 5 |\n",
      "original layer-2 (2  parts):       |26 |19 |\n",
      " summary layer-2 (2  parts):       |13 |14 |\n",
      "original layer-2  (1  parts):       |13 |\n",
      "summary  layer-2  (1  parts):       | 3 |"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'(CNN)James Holmes made his introduction to the world in a Colorado cinema filled with spectators watching a midnight showing of the new Batman movie, \"The Dark Knight Rises,\" in June 2012. In January, he had a beard and eyeglasses. In March 2012, he told a classmate he wanted to kill people, and that he would do so \"when his life was over,\" court documents said.'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goal_num_sents = count_sents(df['highlights'][719])\n",
    "bert_summarize_article_with_log(df['article'][719],\n",
    "        max_bert_input_length=26, # sentences\n",
    "        step_summary_length=13, # sentences\n",
    "        final_summary_length=goal_num_sents, # sentences,\n",
    "        log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc0ed2-87db-4901-b2f2-33bdfcafbaf5",
   "metadata": {},
   "source": [
    "# BERT summarize Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b93ba864-a5c7-40b4-9c76-ecbdf3be5881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11490, 3), (11490, 2))"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_name = 'highlighted'\n",
    "# task_name = 'summarized'\n",
    "# task_name = 'compressed'\n",
    "# task_name = 'abstracted'\n",
    "\n",
    "df_original = pd.read_csv(\"./dat/cnn_dailymail_test.csv\") # 這是原始的dataset但不會用到它\n",
    "df_by_llama = pd.read_csv(f\"./dat/cnn_dailymail_test-{task_name}.csv\")\n",
    "df_original.shape, df_by_llama.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e5ef957a-38b5-4180-b020-f6c5f78af6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BERT summarize:   0%|                                                                                            | 12/11490 [00:21<5:39:15,  1.77s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[219], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     goal_num_sents \u001b[38;5;241m=\u001b[39m count_sents(cnn_hightlights[i])\n\u001b[0;32m---> 11\u001b[0m     summarized_article \u001b[38;5;241m=\u001b[39m \u001b[43mbert_summarize_article\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllama_articles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mmax_bert_input_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m26\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# sentences\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mstep_summary_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# sentences\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mfinal_summary_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgoal_num_sents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# sentences\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     summarized_articles\u001b[38;5;241m.\u001b[39mappend(summarized_article)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[146], line 18\u001b[0m, in \u001b[0;36mbert_summarize_article\u001b[0;34m(article, max_bert_input_length, step_summary_length, final_summary_length)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(num_previous_part\u001b[38;5;241m==\u001b[39mnum_current_part): \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     num_previous_part \u001b[38;5;241m=\u001b[39m num_current_part\n\u001b[0;32m---> 18\u001b[0m final_summary \u001b[38;5;241m=\u001b[39m \u001b[43mBERT_summarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle_parts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_sentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_summary_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_summary\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/summarizer/summary_processor.py:234\u001b[0m, in \u001b[0;36mSummaryProcessor.__call__\u001b[0;34m(self, body, ratio, min_length, max_length, use_first, algorithm, num_sentences, return_as_list)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m     body: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     return_as_list: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    220\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    (utility that wraps around the run function)\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m    Preprocesses the sentences, runs the clusters to find the centroids, then combines the sentences.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m    :return: A summary sentence.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m                    \u001b[49m\u001b[43muse_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_as_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/summarizer/summary_processor.py:202\u001b[0m, in \u001b[0;36mSummaryProcessor.run\u001b[0;34m(self, body, ratio, min_length, max_length, use_first, algorithm, num_sentences, return_as_list)\u001b[0m\n\u001b[1;32m    199\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentence_handler(body, min_length, max_length)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sentences:\n\u001b[0;32m--> 202\u001b[0m     sentences, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_sentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_as_list:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentences\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/summarizer/summary_processor.py:108\u001b[0m, in \u001b[0;36mSummaryProcessor.cluster_runner\u001b[0;34m(self, sentences, ratio, algorithm, use_first, num_sentences)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03mRuns the cluster algorithm based on the hidden state. Returns both the embeddings and sentences.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m:return: A tuple of summarized sentences and embeddings\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m first_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_first:\n\u001b[1;32m    111\u001b[0m     num_sentences \u001b[38;5;241m=\u001b[39m num_sentences \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_sentences \u001b[38;5;28;01melse\u001b[39;00m num_sentences\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/summarizer/transformer_embeddings/bert_embedding.py:173\u001b[0m, in \u001b[0;36mBertEmbedding.__call__\u001b[0;34m(self, content, hidden, reduce_option, hidden_concat)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    159\u001b[0m     content: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m     hidden_concat: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    163\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ndarray:\n\u001b[1;32m    164\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    Create matrix from the embeddings.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    :return: A numpy array matrix of the given content.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_option\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_concat\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/summarizer/transformer_embeddings/bert_embedding.py:151\u001b[0m, in \u001b[0;36mBertEmbedding.create_matrix\u001b[0;34m(self, content, hidden, reduce_option, hidden_concat)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_matrix\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     content: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     hidden_concat: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    140\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ndarray:\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    Create matrix from the embeddings.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    :return: A numpy array matrix of the given content.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([\n\u001b[1;32m    152\u001b[0m         np\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_embeddings(\n\u001b[1;32m    153\u001b[0m             t, hidden\u001b[38;5;241m=\u001b[39mhidden, reduce_option\u001b[38;5;241m=\u001b[39mreduce_option, hidden_concat\u001b[38;5;241m=\u001b[39mhidden_concat\n\u001b[1;32m    154\u001b[0m         )\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m content\n\u001b[1;32m    155\u001b[0m     ])\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/summarizer/transformer_embeddings/bert_embedding.py:152\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_matrix\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     content: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     hidden_concat: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    140\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ndarray:\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    Create matrix from the embeddings.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    :return: A numpy array matrix of the given content.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([\n\u001b[0;32m--> 152\u001b[0m         np\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m            \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_option\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_option\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_concat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_concat\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m content\n\u001b[1;32m    155\u001b[0m     ])\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/summarizer/transformer_embeddings/bert_embedding.py:108\u001b[0m, in \u001b[0;36mBertEmbedding.extract_embeddings\u001b[0;34m(self, text, hidden, reduce_option, hidden_concat)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03mExtracts the embeddings for the given text.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m:return: A torch vector.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m tokens_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_input(text)\n\u001b[0;32m--> 108\u001b[0m pooled, hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens_tensor\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# deprecated temporary keyword functions.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce_option \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconcat_last_4\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    537\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 539\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pytorch_utils.py:236\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 551\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:451\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 451\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SAVE_RESULT_PATH = f'./dat/cnn_dailymail_test-{task_name}-bert.csv'\n",
    "artcle_id = df_by_llama['id']\n",
    "llama_articles = df_by_llama['article']\n",
    "cnn_hightlights = df_original['highlights']\n",
    "\n",
    "max_num = len(cnn_id)\n",
    "summarized_articles = []\n",
    "for i in tqdm(range(max_num), total=max_num, desc='BERT summarize'):\n",
    "    try:\n",
    "        goal_num_sents = count_sents(cnn_hightlights[i])\n",
    "        summarized_article = bert_summarize_article(llama_articles[i],\n",
    "                                                    max_bert_input_length=26, # sentences\n",
    "                                                    step_summary_length=13, # sentences\n",
    "                                                    final_summary_length=goal_num_sents, # sentences\n",
    "                                                   )\n",
    "        summarized_articles.append(summarized_article)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {i}: {e}\")\n",
    "\n",
    "df_bert = pd.DataFrame({\n",
    "    'id': artcle_id,\n",
    "    'article': summarized_articles\n",
    "})\n",
    "df_bert.to_csv(SAVE_RESULT_PATH, index=None)\n",
    "print(\"summarize complete => save to csv complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de566405-2c38-4613-b2ba-488575963b8f",
   "metadata": {},
   "source": [
    "# BERT summarize testing\n",
    "\n",
    "**Following is testing for article summairze by BERT and do ROUGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc10aa05-4cd0-488c-8922-e99cf9eb8be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_scores(reference_text, generated_text):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_text, generated_text)\n",
    "    rouge_1 = scores['rouge1'].fmeasure\n",
    "    rouge_2 = scores['rouge2'].fmeasure\n",
    "    rouge_L = scores['rougeL'].fmeasure\n",
    "    return rouge_1, rouge_2, rouge_L"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
